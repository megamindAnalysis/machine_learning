{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "#import visuals as vs # Import supplementary visualization code visuals.py\n",
    "from sklearn.preprocessing import MinMaxScaler # Import sklearn.preprocessing.StandardScaler\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "pd.set_option(\"display.max_rows\", 1000)\n",
    "pd.set_option(\"display.max_columns\", 1000)\n",
    "#pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (14,15) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "data_tuesday_original = pd.read_csv('data/Tuesday-WorkingHours.pcap_ISCX.csv')\n",
    "#data_tuesday_original = pd.read_csv('data/Wednesday-workingHours.pcap_ISCX.csv')\n",
    "#data_tuesday_original = pd.read_csv('data/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv')\n",
    "#data_tuesday_original = pd.read_csv('data/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv')\n",
    "#data_tuesday_original = pd.read_csv('data/Friday-WorkingHours-Morning.pcap_ISCX.csv')\n",
    "#data_tuesday_original = pd.read_csv('data/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv')\n",
    "#data_tuesday_original = pd.read_csv('data/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     BENIGN       1.00      1.00      1.00    345663\n",
      "FTP-Patator       1.00      1.00      1.00      6364\n",
      "SSH-Patator       1.00      1.00      1.00      4700\n",
      "\n",
      "avg / total       1.00      1.00      1.00    356727\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_tuesday = data_tuesday_original.copy()\n",
    "\n",
    "features = [' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
    "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
    "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
    "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
    "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
    "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
    "       ' Bwd Packet Length Std', \n",
    "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
    "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
    "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
    "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
    "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
    "       ' Bwd Header Length', \n",
    "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
    "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
    "       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n",
    "       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n",
    "       ' ECE Flag Count', ' Average Packet Size',\n",
    "       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n",
    "       ' Fwd Header Length.1', \n",
    "       ' Fwd Avg Bulk Rate',\n",
    "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n",
    "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
    "       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
    "       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n",
    "       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min']\n",
    "\n",
    "# Separating out the features\n",
    "X = data_tuesday.loc[:, features].values\n",
    "# Separating out the target\n",
    "y = data_tuesday.loc[:,[' Label']].values\n",
    "# Standardizing the features\n",
    "#X = StandardScaler().fit_transform(X)\n",
    "\n",
    "'''train_test_split'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "'''scale data'''\n",
    "scaler = StandardScaler()\n",
    "# Fit on training set only.\n",
    "scaler.fit(X_train)\n",
    "# Apply transform to both the training set and the test set.\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "'''PCA'''\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(.95)\n",
    "pca.fit(X_train)\n",
    "#pca.n_components_\n",
    "X_train = pca.transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "'''Decision Tree Classifier'''\n",
    "from sklearn import tree\n",
    "#dt = tree.DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "#dt.predict(X_test[0].reshape(1,-1))\n",
    "dt.score(X_test, y_test)\n",
    "\n",
    "'''Metrics'''\n",
    "from sklearn import metrics\n",
    "classification = metrics.classification_report(y_train, dt.predict(X_train))\n",
    "print(\"Classification report:\" \"\\n\", classification) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
